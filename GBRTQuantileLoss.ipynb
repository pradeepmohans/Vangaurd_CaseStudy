{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "023cd01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_pinball_loss, make_scorer\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b744521",
   "metadata": {},
   "source": [
    "## Read Train and Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0e2ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a06fa04",
   "metadata": {},
   "source": [
    "## Check for Missing values and replace with the most frequent value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa1da70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 0\n",
       "wi                 0\n",
       "year               0\n",
       "month              0\n",
       "age                0\n",
       "education          0\n",
       "familysize         0\n",
       "urban              0\n",
       "race               0\n",
       "region          6994\n",
       "state          95742\n",
       "marital            0\n",
       "occupation    233483\n",
       "income             0\n",
       "expense            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c20dcd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "wi            0\n",
       "year          0\n",
       "month         0\n",
       "age           0\n",
       "education     0\n",
       "familysize    0\n",
       "urban         0\n",
       "race          0\n",
       "region        0\n",
       "state         0\n",
       "marital       0\n",
       "occupation    0\n",
       "income        0\n",
       "expense       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.fillna(train.mode().iloc[0].astype('int64'))\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "032aedf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                0\n",
       "year              0\n",
       "month             0\n",
       "age               0\n",
       "education         0\n",
       "familysize        0\n",
       "urban             0\n",
       "race              0\n",
       "region         1826\n",
       "state         24016\n",
       "marital           0\n",
       "occupation    58171\n",
       "income            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9524af33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "year          0\n",
       "month         0\n",
       "age           0\n",
       "education     0\n",
       "familysize    0\n",
       "urban         0\n",
       "race          0\n",
       "region        0\n",
       "state         0\n",
       "marital       0\n",
       "occupation    0\n",
       "income        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test.fillna(test.mode().iloc[0].astype('int64'))\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b60b668",
   "metadata": {},
   "source": [
    "## Identify categorical, ordinal and numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf00f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'wi',\n",
       " 'year',\n",
       " 'month',\n",
       " 'age',\n",
       " 'education',\n",
       " 'familysize',\n",
       " 'urban',\n",
       " 'race',\n",
       " 'region',\n",
       " 'state',\n",
       " 'marital',\n",
       " 'occupation',\n",
       " 'income',\n",
       " 'expense']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = train.columns\n",
    "columns  = columns.to_list()\n",
    "categorical_columns=['urban','race','region','state','marital','occupation']\n",
    "ordinal_columns=['education']\n",
    "numeric_columns=['age','familysize','income']\n",
    "weight_column=['wi']\n",
    "target=['expense']\n",
    "id_column=['id']\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27671c08",
   "metadata": {},
   "source": [
    "## Convert types of various columns in train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caaa8a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert cateogrical columns to 'category' type\n",
    "cat_types={c:'category' for c in categorical_columns}\n",
    "train = train.astype(cat_types)\n",
    "#Convert ordinal column to category type\n",
    "train['education'] = train['education'].astype('category')\n",
    "#Get different categories for ordinal column\n",
    "train_ord_categories = train.education.unique().tolist()\n",
    "train_ord_categories.sort()\n",
    "train['education'] = train['education'].cat.set_categories(train_ord_categories,ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f28d5647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               int64\n",
       "year             int64\n",
       "month            int64\n",
       "age              int64\n",
       "education     category\n",
       "familysize       int64\n",
       "urban         category\n",
       "race          category\n",
       "region        category\n",
       "state         category\n",
       "marital       category\n",
       "occupation    category\n",
       "income           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert cateogrical columns to 'category' type\n",
    "cat_types={c:'category' for c in categorical_columns}\n",
    "test = test.astype(cat_types)\n",
    "#Convert ordinal column to category type\n",
    "test['education'] = test['education'].astype('category')\n",
    "#Get different categories for ordinal column\n",
    "test_ord_categories = test.education.unique().tolist()\n",
    "test_ord_categories.sort()\n",
    "test['education'] = test['education'].cat.set_categories(test_ord_categories,ordered=True)\n",
    "test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a047c295",
   "metadata": {},
   "source": [
    "## Create Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "695a05a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "a_train,a_validation = train_test_split(train,test_size=0.2)\n",
    "train = pd.DataFrame(a_train,columns=columns)\n",
    "validation = pd.DataFrame(a_validation,columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f60b476",
   "metadata": {},
   "source": [
    "## Create X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3909ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[categorical_columns+ordinal_columns+numeric_columns]\n",
    "y_train = train[target]\n",
    "sample_weights_train=train[weight_column].to_numpy()\n",
    "sample_weights_train=sample_weights_train.reshape(-1)\n",
    "X_val = validation[categorical_columns+ordinal_columns+numeric_columns]\n",
    "y_val = validation[target]\n",
    "sample_weights_validation=validation[weight_column].to_numpy()\n",
    "sample_weights_validation = sample_weights_validation.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca353a",
   "metadata": {},
   "source": [
    "## Create X_test for final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7229b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[categorical_columns+ordinal_columns+numeric_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686415e",
   "metadata": {},
   "source": [
    "## Create preprocessor and column transformer for categorical,numerical and ordinal columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5cc42fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "numerical_preprocessor = StandardScaler()\n",
    "ordinal_preprocessor = OrdinalEncoder(dtype=int)\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"one-hot-encoder\", categorical_preprocessor, categorical_columns),\n",
    "        (\"ordinal-encoder\", ordinal_preprocessor,ordinal_columns),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50d0b9",
   "metadata": {},
   "source": [
    "## Create estimators and scorers for each quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaf2735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.005,0.025,0.165,0.25,0.5,0.75,0.835,0.975,0.995]\n",
    "estimators={}\n",
    "scorers = {}\n",
    "for index,q in enumerate(quantiles):\n",
    "    estimators[\"q\"+str(index+1)] = Pipeline([('preprocessor',preprocessor),('model',GradientBoostingRegressor(loss=\"quantile\",alpha=q,random_state=42,verbose=2))])\n",
    "    scorers[\"q\"+str(index+1)] = make_scorer(mean_pinball_loss,alpha=q,greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b3006",
   "metadata": {},
   "source": [
    "## Create Hyperparameters for search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e173dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"model__learning_rate\":loguniform(0.001,1).rvs(size=10)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e1244c",
   "metadata": {},
   "source": [
    "## Create search_cv for each quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f1ec499",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_cv = {}\n",
    "for q in estimators.keys():\n",
    "    search_cv[q] = GridSearchCV(estimator=estimators[q],\n",
    "                                param_grid=hyperparameters,\n",
    "                                scoring=scorers[q],\n",
    "                                n_jobs=8,\n",
    "                                verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5f6f1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching best Hyperparameters for q1\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1          20.9298            1.39m\n",
      "         2          20.9298            1.33m\n",
      "         3          20.9298            1.25m\n",
      "         4          20.9298            1.22m\n",
      "         5          20.9298            1.18m\n",
      "         6          20.9298            1.17m\n",
      "         7          20.9298            1.16m\n",
      "         8          20.9298            1.16m\n",
      "         9          20.9298            1.16m\n",
      "        10          20.9298            1.14m\n",
      "        11          20.9298            1.15m\n",
      "        12          20.9298            1.13m\n",
      "        13          20.9298            1.11m\n",
      "        14          20.9298            1.09m\n",
      "        15          20.9298            1.08m\n",
      "        16          20.9298            1.07m\n",
      "        17          20.9298            1.05m\n",
      "        18          20.9298            1.03m\n",
      "        19          20.9298            1.02m\n",
      "        20          20.9298            1.01m\n",
      "        21          20.9298           59.85s\n",
      "        22          20.9298           59.13s\n",
      "        23          20.9298           58.37s\n",
      "        24          20.9298           57.48s\n",
      "        25          20.9298           56.77s\n",
      "        26          20.9298           55.94s\n",
      "        27          20.9298           55.05s\n",
      "        28          20.9298           54.11s\n",
      "        29          20.9298           53.45s\n",
      "        30          20.9298           52.69s\n",
      "        31          20.9298           51.88s\n",
      "        32          20.9298           51.19s\n",
      "        33          20.9298           50.45s\n",
      "        34          20.9298           49.76s\n",
      "        35          20.9298           49.06s\n",
      "        36          20.9298           48.22s\n",
      "        37          20.9298           47.53s\n",
      "        38          20.9298           46.81s\n",
      "        39          20.9298           46.04s\n",
      "        40          20.9298           45.27s\n",
      "        41          20.9298           44.49s\n",
      "        42          20.9298           43.67s\n",
      "        43          20.9298           42.89s\n",
      "        44          20.9298           42.21s\n",
      "        45          20.9298           41.41s\n",
      "        46          20.9298           40.69s\n",
      "        47          20.9298           39.96s\n",
      "        48          20.9298           39.18s\n",
      "        49          20.9298           38.35s\n",
      "        50          20.9298           37.52s\n",
      "        51          20.9298           36.74s\n",
      "        52          20.9298           35.96s\n",
      "        53          20.9298           35.19s\n",
      "        54          20.9298           34.42s\n",
      "        55          20.9298           33.66s\n",
      "        56          20.9298           32.87s\n",
      "        57          20.9298           32.08s\n",
      "        58          20.9298           31.30s\n",
      "        59          20.9298           30.58s\n",
      "        60          20.9298           29.81s\n",
      "        61          20.9298           29.00s\n",
      "        62          20.9298           28.32s\n",
      "        63          20.9298           27.55s\n",
      "        64          20.9298           26.81s\n",
      "        65          20.9298           26.06s\n",
      "        66          20.9298           25.33s\n",
      "        67          20.9298           24.60s\n",
      "        68          20.9298           23.85s\n",
      "        69          20.9298           23.11s\n",
      "        70          20.9298           22.36s\n",
      "        71          20.9298           21.60s\n",
      "        72          20.9298           20.84s\n",
      "        73          20.9298           20.10s\n",
      "        74          20.9298           19.35s\n",
      "        75          20.9298           18.62s\n",
      "        76          20.9298           17.86s\n",
      "        77          20.9298           17.11s\n",
      "        78          20.9298           16.40s\n",
      "        79          20.9298           15.65s\n",
      "        80          20.9298           14.95s\n",
      "        81          20.9298           14.22s\n",
      "        82          20.9298           13.49s\n",
      "        83          20.9298           12.75s\n",
      "        84          20.9298           12.00s\n",
      "        85          20.9298           11.24s\n",
      "        86          20.9298           10.49s\n",
      "        87          20.9298            9.74s\n",
      "        88          20.9298            8.99s\n",
      "        89          20.9298            8.25s\n",
      "        90          20.9298            7.50s\n",
      "        91          20.9298            6.75s\n",
      "        92          20.9298            6.00s\n",
      "        93          20.9298            5.25s\n",
      "        94          20.9298            4.50s\n",
      "        95          20.9298            3.75s\n",
      "        96          20.9298            3.00s\n",
      "        97          20.9298            2.25s\n",
      "        98          20.9298            1.50s\n",
      "        99          20.9298            0.75s\n",
      "       100          20.9298            0.00s\n",
      "Searching best Hyperparameters for q2\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1          78.1322            1.24m\n",
      "         2          78.1143            1.22m\n",
      "         3          78.0704            1.19m\n",
      "         4          78.0625            1.18m\n",
      "         5          78.0556            1.16m\n",
      "         6          78.0544            1.16m\n",
      "         7          78.0538            1.16m\n",
      "         8          78.0535            1.14m\n",
      "         9          78.0533            1.12m\n",
      "        10          78.0532            1.11m\n",
      "        11          78.0532            1.09m\n",
      "        12          78.0531            1.08m\n",
      "        13          78.0531            1.07m\n",
      "        14          78.0531            1.06m\n",
      "        15          78.0531            1.04m\n",
      "        16          78.0531            1.04m\n",
      "        17          78.0531            1.02m\n",
      "        18          78.0531            1.01m\n",
      "        19          78.0531            1.00m\n",
      "        20          78.0531           59.56s\n",
      "        21          78.0531           59.15s\n",
      "        22          78.0531           58.36s\n",
      "        23          78.0531           57.55s\n",
      "        24          78.0531           57.07s\n",
      "        25          78.0531           56.50s\n",
      "        26          78.0531           55.57s\n",
      "        27          78.0531           54.85s\n",
      "        28          78.0531           54.21s\n",
      "        29          78.0531           53.54s\n",
      "        30          78.0531           52.66s\n",
      "        31          78.0531           51.88s\n",
      "        32          78.0531           51.19s\n",
      "        33          78.0531           50.39s\n",
      "        34          78.0531           49.64s\n",
      "        35          78.0531           48.91s\n",
      "        36          78.0531           48.06s\n",
      "        37          78.0531           47.33s\n",
      "        38          78.0531           46.59s\n",
      "        39          78.0531           45.82s\n",
      "        40          78.0531           45.05s\n",
      "        41          78.0531           44.17s\n",
      "        42          78.0531           43.50s\n",
      "        43          78.0531           42.74s\n",
      "        44          78.0531           41.93s\n",
      "        45          78.0531           41.19s\n",
      "        46          78.0531           40.46s\n",
      "        47          78.0531           39.71s\n",
      "        48          78.0531           38.92s\n",
      "        49          78.0531           38.21s\n",
      "        50          78.0531           37.52s\n",
      "        51          78.0531           36.80s\n",
      "        52          78.0531           36.03s\n",
      "        53          78.0531           35.30s\n",
      "        54          78.0531           34.54s\n",
      "        55          78.0531           33.74s\n",
      "        56          78.0531           33.01s\n",
      "        57          78.0531           32.26s\n",
      "        58          78.0531           31.49s\n",
      "        59          78.0531           30.71s\n",
      "        60          78.0531           30.02s\n",
      "        61          78.0531           29.26s\n",
      "        62          78.0531           28.48s\n",
      "        63          78.0531           27.79s\n",
      "        64          78.0531           27.11s\n",
      "        65          78.0531           26.40s\n",
      "        66          78.0531           25.70s\n",
      "        67          78.0531           24.95s\n",
      "        68          78.0531           24.22s\n",
      "        69          78.0531           23.47s\n",
      "        70          78.0531           22.72s\n",
      "        71          78.0531           21.97s\n",
      "        72          78.0531           21.22s\n",
      "        73          78.0531           20.47s\n",
      "        74          78.0531           19.74s\n",
      "        75          78.0531           19.03s\n",
      "        76          78.0531           18.29s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        77          78.0531           17.54s\n",
      "        78          78.0531           16.79s\n",
      "        79          78.0531           16.03s\n",
      "        80          78.0531           15.28s\n",
      "        81          78.0531           14.54s\n",
      "        82          78.0531           13.80s\n",
      "        83          78.0531           13.09s\n",
      "        84          78.0531           12.39s\n",
      "        85          78.0531           11.68s\n",
      "        86          78.0531           10.95s\n",
      "        87          78.0531           10.21s\n",
      "        88          78.0531            9.48s\n",
      "        89          78.0531            8.74s\n",
      "        90          78.0531            7.98s\n",
      "        91          78.0531            7.21s\n",
      "        92          78.0531            6.44s\n",
      "        93          78.0531            5.66s\n",
      "        94          78.0531            4.87s\n",
      "        95          78.0531            4.07s\n",
      "        96          78.0531            3.27s\n",
      "        97          78.0531            2.46s\n",
      "        98          78.0531            1.65s\n",
      "        99          78.0531            0.82s\n",
      "       100          78.0531            0.00s\n",
      "Searching best Hyperparameters for q3\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         389.9411            1.42m\n",
      "         2         383.7013            1.29m\n",
      "         3         381.3600            1.26m\n",
      "         4         379.5621            1.25m\n",
      "         5         378.7306            1.20m\n",
      "         6         377.9069            1.18m\n",
      "         7         377.4507            1.16m\n",
      "         8         377.1330            1.14m\n",
      "         9         376.9313            1.11m\n",
      "        10         376.7493            1.09m\n",
      "        11         376.5892            1.09m\n",
      "        12         376.5156            1.08m\n",
      "        13         376.3278            1.06m\n",
      "        14         376.1355            1.04m\n",
      "        15         376.0313            1.03m\n",
      "        16         375.9344            1.02m\n",
      "        17         375.8492            1.01m\n",
      "        18         375.7880           59.52s\n",
      "        19         375.7059           58.54s\n",
      "        20         375.6544           57.80s\n",
      "        21         375.6033           56.91s\n",
      "        22         375.5187           56.00s\n",
      "        23         375.4739           55.34s\n",
      "        24         375.3834           54.49s\n",
      "        25         375.3194           53.75s\n",
      "        26         375.2760           53.09s\n",
      "        27         375.2191           52.25s\n",
      "        28         375.1818           51.38s\n",
      "        29         375.0423           50.58s\n",
      "        30         375.0161           49.77s\n",
      "        31         374.9799           49.07s\n",
      "        32         374.9520           48.39s\n",
      "        33         374.9297           47.59s\n",
      "        34         374.9024           46.81s\n",
      "        35         374.8784           46.08s\n",
      "        36         374.8522           45.28s\n",
      "        37         374.8235           44.59s\n",
      "        38         374.7916           43.83s\n",
      "        39         374.7537           43.06s\n",
      "        40         374.7343           42.37s\n",
      "        41         374.6193           41.76s\n",
      "        42         374.5336           41.03s\n",
      "        43         374.4815           40.29s\n",
      "        44         374.4527           39.53s\n",
      "        45         374.4331           38.81s\n",
      "        46         374.3829           38.05s\n",
      "        47         374.3692           37.31s\n",
      "        48         374.3020           36.57s\n",
      "        49         374.2773           35.81s\n",
      "        50         374.2551           39.32s\n",
      "        51         374.2418           38.43s\n",
      "        52         374.2102           37.65s\n",
      "        53         374.1919           36.78s\n",
      "        54         374.1768           35.93s\n",
      "        55         374.1671           35.13s\n",
      "        56         374.1366           34.30s\n",
      "        57         374.1245           33.52s\n",
      "        58         374.1051           32.67s\n",
      "        59         374.0827           31.90s\n",
      "        60         374.0650           31.11s\n",
      "        61         374.0313           30.28s\n",
      "        62         374.0162           29.46s\n",
      "        63         374.0029           28.66s\n",
      "        64         373.9876           27.84s\n",
      "        65         373.9784           27.00s\n",
      "        66         373.9495           26.20s\n",
      "        67         373.9337           25.40s\n",
      "        68         373.9113           24.60s\n",
      "        69         373.8921           23.81s\n",
      "        70         373.8660           22.99s\n",
      "        71         373.8508           22.20s\n",
      "        72         373.8389           21.42s\n",
      "        73         373.8034           20.63s\n",
      "        74         373.7692           19.85s\n",
      "        75         373.7498           19.07s\n",
      "        76         373.7317           18.30s\n",
      "        77         373.7148           17.54s\n",
      "        78         373.6995           16.79s\n",
      "        79         373.6903           16.03s\n",
      "        80         373.6725           15.26s\n",
      "        81         373.6661           14.48s\n",
      "        82         373.6590           13.72s\n",
      "        83         373.6488           12.94s\n",
      "        84         373.6365           12.17s\n",
      "        85         373.6269           11.40s\n",
      "        86         373.6086           10.63s\n",
      "        87         373.6014            9.86s\n",
      "        88         373.5771            9.10s\n",
      "        89         373.5675            8.34s\n",
      "        90         373.5374            7.58s\n",
      "        91         373.5015            6.83s\n",
      "        92         373.4666            6.07s\n",
      "        93         373.4403            5.31s\n",
      "        94         373.4260            4.55s\n",
      "        95         373.4151            3.79s\n",
      "        96         373.4023            3.03s\n",
      "        97         373.3939            2.27s\n",
      "        98         373.3752            1.51s\n",
      "        99         373.3621            0.76s\n",
      "       100         373.3537            0.00s\n",
      "Searching best Hyperparameters for q4\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         536.4060            1.26m\n",
      "         2         525.3497            1.22m\n",
      "         3         521.2415            1.23m\n",
      "         4         518.3506            1.26m\n",
      "         5         516.8302            1.21m\n",
      "         6         515.9659            1.22m\n",
      "         7         515.3860            1.20m\n",
      "         8         514.5121            1.18m\n",
      "         9         514.0174            1.16m\n",
      "        10         513.6536            1.15m\n",
      "        11         513.2836            1.15m\n",
      "        12         513.0490            1.13m\n",
      "        13         512.8282            1.12m\n",
      "        14         512.6860            1.10m\n",
      "        15         512.4414            1.09m\n",
      "        16         512.3328            1.08m\n",
      "        17         512.2187            1.06m\n",
      "        18         512.0711            1.05m\n",
      "        19         511.9393            1.04m\n",
      "        20         511.8518            1.02m\n",
      "        21         511.7261            1.00m\n",
      "        22         511.6382           59.46s\n",
      "        23         511.5148           58.68s\n",
      "        24         511.4032           58.00s\n",
      "        25         511.3307           57.05s\n",
      "        26         511.2608           56.18s\n",
      "        27         511.1953           55.40s\n",
      "        28         511.1308           54.35s\n",
      "        29         511.0151           53.66s\n",
      "        30         510.9577           52.79s\n",
      "        31         510.8976           51.85s\n",
      "        32         510.8491           51.14s\n",
      "        33         510.8044           50.32s\n",
      "        34         510.7497           49.48s\n",
      "        35         510.6936           48.64s\n",
      "        36         510.5998           47.81s\n",
      "        37         510.5468           47.28s\n",
      "        38         510.5226           46.49s\n",
      "        39         510.4822           45.71s\n",
      "        40         510.4486           45.03s\n",
      "        41         510.4035           44.32s\n",
      "        42         510.3640           43.55s\n",
      "        43         510.3415           42.74s\n",
      "        44         510.3037           41.99s\n",
      "        45         510.2864           41.15s\n",
      "        46         510.2657           40.35s\n",
      "        47         510.2383           39.56s\n",
      "        48         510.2041           38.81s\n",
      "        49         510.1798           38.20s\n",
      "        50         510.1414           37.61s\n",
      "        51         510.0879           37.10s\n",
      "        52         510.0684           36.51s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        53         510.0076           35.84s\n",
      "        54         509.8611           35.20s\n",
      "        55         509.8132           34.50s\n",
      "        56         509.7918           33.86s\n",
      "        57         509.7658           33.11s\n",
      "        58         509.7362           32.38s\n",
      "        59         509.7191           31.67s\n",
      "        60         509.6701           30.87s\n",
      "        61         509.6636           30.15s\n",
      "        62         509.6335           29.48s\n",
      "        63         509.6184           28.76s\n",
      "        64         509.6090           27.92s\n",
      "        65         509.5827           27.13s\n",
      "        66         509.5712           26.37s\n",
      "        67         509.5481           25.61s\n",
      "        68         509.5371           24.88s\n",
      "        69         509.4972           24.13s\n",
      "        70         509.4180           23.34s\n",
      "        71         509.3759           22.56s\n",
      "        72         509.3652           21.73s\n",
      "        73         509.3355           20.94s\n",
      "        74         509.3227           20.14s\n",
      "        75         509.2190           19.39s\n",
      "        76         509.1628           18.60s\n",
      "        77         509.0999           17.81s\n",
      "        78         509.0357           17.04s\n",
      "        79         509.0145           16.28s\n",
      "        80         508.9956           15.52s\n",
      "        81         508.9508           14.73s\n",
      "        82         508.9216           13.93s\n",
      "        83         508.8640           13.16s\n",
      "        84         508.8441           12.37s\n",
      "        85         508.8327           11.59s\n",
      "        86         508.8092           10.81s\n",
      "        87         508.8008           10.03s\n",
      "        88         508.7942            9.26s\n",
      "        89         508.7758            8.48s\n",
      "        90         508.7647            7.71s\n",
      "        91         508.7467            6.93s\n",
      "        92         508.7399            6.15s\n",
      "        93         508.7298            5.38s\n",
      "        94         508.7064            4.61s\n",
      "        95         508.6847            3.84s\n",
      "        96         508.6727            3.07s\n",
      "        97         508.6607            2.30s\n",
      "        98         508.6546            1.53s\n",
      "        99         508.6421            0.77s\n",
      "       100         508.6339            0.00s\n",
      "Searching best Hyperparameters for q5\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         830.1099            1.34m\n",
      "         2         808.8621            1.28m\n",
      "         3         800.4646            1.26m\n",
      "         4         795.5157            1.26m\n",
      "         5         792.9435            1.22m\n",
      "         6         790.7906            1.20m\n",
      "         7         789.5138            1.19m\n",
      "         8         788.5593            1.18m\n",
      "         9         787.7940            1.17m\n",
      "        10         787.2408            1.15m\n",
      "        11         786.7396            1.14m\n",
      "        12         786.2635            1.13m\n",
      "        13         785.8558            1.11m\n",
      "        14         785.6015            1.09m\n",
      "        15         785.2308            1.09m\n",
      "        16         784.9695            1.07m\n",
      "        17         784.7227            1.06m\n",
      "        18         784.4338            1.05m\n",
      "        19         784.2531            1.04m\n",
      "        20         783.8971            1.03m\n",
      "        21         783.7238            1.01m\n",
      "        22         783.4811           59.73s\n",
      "        23         783.2898           59.16s\n",
      "        24         783.1212           58.57s\n",
      "        25         782.9812           57.72s\n",
      "        26         782.8423           56.74s\n",
      "        27         782.7020           55.85s\n",
      "        28         782.5746           54.89s\n",
      "        29         782.4571           54.14s\n",
      "        30         782.3619           53.27s\n",
      "        31         782.2875           53.05s\n",
      "        32         782.2385           52.68s\n",
      "        33         782.1642           52.25s\n",
      "        34         782.0460           51.72s\n",
      "        35         781.6332           51.29s\n",
      "        36         781.5694           50.71s\n",
      "        37         781.5031           50.23s\n",
      "        38         781.4287           49.56s\n",
      "        39         781.3799           49.08s\n",
      "        40         781.1257           48.53s\n",
      "        41         781.0471           47.88s\n",
      "        42         780.9942           47.28s\n",
      "        43         780.9244           46.53s\n",
      "        44         780.8782           45.89s\n",
      "        45         780.8303           45.11s\n",
      "        46         780.7886           44.45s\n",
      "        47         780.7062           43.79s\n",
      "        48         780.6566           42.95s\n",
      "        49         780.6183           42.22s\n",
      "        50         780.5850           41.31s\n",
      "        51         780.5162           40.51s\n",
      "        52         780.4871           39.74s\n",
      "        53         780.4489           38.96s\n",
      "        54         780.4176           38.13s\n",
      "        55         780.3911           37.41s\n",
      "        56         780.3455           36.63s\n",
      "        57         780.3028           36.04s\n",
      "        58         780.2085           35.40s\n",
      "        59         780.1333           34.75s\n",
      "        60         779.9379           33.99s\n",
      "        61         779.8380           33.23s\n",
      "        62         779.8082           32.39s\n",
      "        63         779.7426           31.58s\n",
      "        64         779.7239           30.78s\n",
      "        65         779.6774           29.94s\n",
      "        66         779.6601           29.07s\n",
      "        67         779.5723           28.23s\n",
      "        68         779.5500           27.36s\n",
      "        69         779.5217           26.44s\n",
      "        70         779.5014           25.52s\n",
      "        71         779.4819           24.68s\n",
      "        72         779.4654           23.77s\n",
      "        73         779.4321           22.91s\n",
      "        74         779.3680           22.04s\n",
      "        75         779.3193           21.17s\n",
      "        76         779.2966           20.30s\n",
      "        77         779.2694           19.43s\n",
      "        78         779.2573           18.54s\n",
      "        79         779.2326           17.67s\n",
      "        80         779.2188           16.80s\n",
      "        81         779.2002           15.95s\n",
      "        82         779.0808           15.08s\n",
      "        83         778.9341           14.23s\n",
      "        84         778.7666           13.38s\n",
      "        85         778.7484           12.52s\n",
      "        86         778.7339           11.67s\n",
      "        87         778.7073           10.84s\n",
      "        88         778.6797           10.01s\n",
      "        89         778.6675            9.18s\n",
      "        90         778.6268            8.34s\n",
      "        91         778.5816            7.51s\n",
      "        92         778.5504            6.68s\n",
      "        93         778.5386            5.84s\n",
      "        94         778.5247            5.00s\n",
      "        95         778.5049            4.17s\n",
      "        96         778.4986            3.34s\n",
      "        97         778.4803            2.50s\n",
      "        98         778.4684            1.67s\n",
      "        99         778.4510            0.83s\n",
      "       100         778.4403            0.00s\n",
      "Searching best Hyperparameters for q6\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         884.6124            1.52m\n",
      "         2         861.0374            1.47m\n",
      "         3         851.2280            1.39m\n",
      "         4         845.2803            1.32m\n",
      "         5         842.1612            1.31m\n",
      "         6         839.5805            1.30m\n",
      "         7         838.1420            1.28m\n",
      "         8         836.8700            1.25m\n",
      "         9         836.0020            1.23m\n",
      "        10         835.1444            1.21m\n",
      "        11         834.5086            1.19m\n",
      "        12         833.9497            1.17m\n",
      "        13         833.4081            1.16m\n",
      "        14         833.1071            1.15m\n",
      "        15         832.7830            1.14m\n",
      "        16         832.5632            1.12m\n",
      "        17         832.2957            1.10m\n",
      "        18         831.2117            1.10m\n",
      "        19         830.9655            1.09m\n",
      "        20         830.7037            1.08m\n",
      "        21         830.5385            1.07m\n",
      "        22         830.2291            1.06m\n",
      "        23         829.9427            1.05m\n",
      "        24         829.8232            1.04m\n",
      "        25         829.7040            1.03m\n",
      "        26         829.5547            1.02m\n",
      "        27         829.4339            1.00m\n",
      "        28         829.2251           59.37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        29         829.1062           58.53s\n",
      "        30         828.9484           57.77s\n",
      "        31         828.8368           56.89s\n",
      "        32         828.7117           56.06s\n",
      "        33         828.6023           55.21s\n",
      "        34         828.5398           54.33s\n",
      "        35         828.4051           53.56s\n",
      "        36         828.3243           52.81s\n",
      "        37         828.2647           51.89s\n",
      "        38         828.1322           50.97s\n",
      "        39         828.0462           50.12s\n",
      "        40         827.9896           49.29s\n",
      "        41         827.7840           48.39s\n",
      "        42         827.6833           47.54s\n",
      "        43         827.5856           46.90s\n",
      "        44         827.5142           46.05s\n",
      "        45         827.4126           45.20s\n",
      "        46         827.3761           44.55s\n",
      "        47         827.3385           43.81s\n",
      "        48         827.2619           42.91s\n",
      "        49         827.2290           42.20s\n",
      "        50         827.1930           41.27s\n",
      "        51         827.1651           40.47s\n",
      "        52         827.0975           39.59s\n",
      "        53         827.0254           38.72s\n",
      "        54         826.8877           38.22s\n",
      "        55         826.8436           37.36s\n",
      "        56         826.7233           36.59s\n",
      "        57         826.5992           35.94s\n",
      "        58         826.5380           35.12s\n",
      "        59         826.4340           34.44s\n",
      "        60         826.4003           33.57s\n",
      "        61         826.3578           32.80s\n",
      "        62         826.3040           32.03s\n",
      "        63         826.2389           31.22s\n",
      "        64         826.2084           30.46s\n",
      "        65         826.1719           29.71s\n",
      "        66         826.1108           28.95s\n",
      "        67         826.0595           28.27s\n",
      "        68         825.9568           27.52s\n",
      "        69         825.9199           26.67s\n",
      "        70         825.8837           25.89s\n",
      "        71         825.7996           25.07s\n",
      "        72         825.6448           24.23s\n",
      "        73         825.6019           23.50s\n",
      "        74         825.5741           22.67s\n",
      "        75         825.5046           21.89s\n",
      "        76         825.4729           21.13s\n",
      "        77         825.2569           20.30s\n",
      "        78         825.2018           19.45s\n",
      "        79         825.1703           18.57s\n",
      "        80         825.1215           17.68s\n",
      "        81         825.0895           16.81s\n",
      "        82         824.9330           15.93s\n",
      "        83         824.8764           15.05s\n",
      "        84         824.8165           14.17s\n",
      "        85         824.7865           13.30s\n",
      "        86         824.7504           12.41s\n",
      "        87         824.7222           11.53s\n",
      "        88         824.7030           10.66s\n",
      "        89         824.6845            9.77s\n",
      "        90         824.6731            8.90s\n",
      "        91         824.6385            8.02s\n",
      "        92         824.6158            7.12s\n",
      "        93         824.6041            6.23s\n",
      "        94         824.5729            5.35s\n",
      "        95         824.5412            4.45s\n",
      "        96         824.4976            3.56s\n",
      "        97         824.4802            2.67s\n",
      "        98         824.4589            1.78s\n",
      "        99         824.3809            0.89s\n",
      "       100         824.3514            0.00s\n",
      "Searching best Hyperparameters for q7\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         812.8797            1.28m\n",
      "         2         791.4539            1.37m\n",
      "         3         782.0125            1.32m\n",
      "         4         776.5992            1.29m\n",
      "         5         773.8087            1.28m\n",
      "         6         771.8058            1.25m\n",
      "         7         769.9172            1.24m\n",
      "         8         768.6833            1.22m\n",
      "         9         767.9889            1.22m\n",
      "        10         767.3521            1.21m\n",
      "        11         766.5581            1.19m\n",
      "        12         766.1470            1.18m\n",
      "        13         765.8377            1.16m\n",
      "        14         765.5750            1.15m\n",
      "        15         765.3274            1.14m\n",
      "        16         765.0951            1.12m\n",
      "        17         764.5972            1.12m\n",
      "        18         764.4165            1.11m\n",
      "        19         764.1652            1.10m\n",
      "        20         764.0083            1.08m\n",
      "        21         763.8401            1.07m\n",
      "        22         763.6447            1.05m\n",
      "        23         763.4582            1.05m\n",
      "        24         763.3625            1.03m\n",
      "        25         763.1909            1.02m\n",
      "        26         763.0879           59.84s\n",
      "        27         762.9025           58.98s\n",
      "        28         762.7813           57.97s\n",
      "        29         762.6693           57.03s\n",
      "        30         762.1597           56.16s\n",
      "        31         762.0619           55.56s\n",
      "        32         761.9933           55.07s\n",
      "        33         761.9069           54.21s\n",
      "        34         761.8376           53.37s\n",
      "        35         761.7826           52.50s\n",
      "        36         761.6968           51.56s\n",
      "        37         761.6386           50.87s\n",
      "        38         761.5662           50.12s\n",
      "        39         761.5065           49.25s\n",
      "        40         761.2052           48.49s\n",
      "        41         761.0833           47.62s\n",
      "        42         760.9841           46.84s\n",
      "        43         760.9045           46.04s\n",
      "        44         760.8674           45.20s\n",
      "        45         760.8103           44.35s\n",
      "        46         760.6535           43.56s\n",
      "        47         760.5989           42.73s\n",
      "        48         760.5571           41.92s\n",
      "        49         760.4838           41.20s\n",
      "        50         760.4374           40.65s\n",
      "        51         760.3500           39.98s\n",
      "        52         760.0624           39.20s\n",
      "        53         760.0194           38.43s\n",
      "        54         759.9875           37.55s\n",
      "        55         759.9625           36.66s\n",
      "        56         759.9285           35.96s\n",
      "        57         759.7864           35.26s\n",
      "        58         759.7434           34.56s\n",
      "        59         759.6916           33.80s\n",
      "        60         759.6606           32.97s\n",
      "        61         759.5549           32.19s\n",
      "        62         759.5070           31.32s\n",
      "        63         759.4710           30.44s\n",
      "        64         759.4305           29.59s\n",
      "        65         759.3755           28.74s\n",
      "        66         759.3326           27.87s\n",
      "        67         759.2938           26.99s\n",
      "        68         759.2533           26.16s\n",
      "        69         759.2240           25.36s\n",
      "        70         759.1957           24.52s\n",
      "        71         759.1118           23.66s\n",
      "        72         759.0631           22.82s\n",
      "        73         759.0404           21.99s\n",
      "        74         758.9989           21.16s\n",
      "        75         758.9798           20.31s\n",
      "        76         758.9622           19.47s\n",
      "        77         758.9192           18.65s\n",
      "        78         758.8813           17.85s\n",
      "        79         758.8381           17.02s\n",
      "        80         758.7719           16.21s\n",
      "        81         758.7388           15.38s\n",
      "        82         758.6654           14.57s\n",
      "        83         758.6482           13.76s\n",
      "        84         758.5976           12.93s\n",
      "        85         758.5553           12.11s\n",
      "        86         758.5420           11.30s\n",
      "        87         758.5105           10.51s\n",
      "        88         758.4609            9.70s\n",
      "        89         758.4404            8.87s\n",
      "        90         758.4028            8.06s\n",
      "        91         758.3888            7.25s\n",
      "        92         758.3716            6.44s\n",
      "        93         758.3090            5.63s\n",
      "        94         758.2541            4.82s\n",
      "        95         758.2238            4.02s\n",
      "        96         758.2016            3.21s\n",
      "        97         758.1968            2.41s\n",
      "        98         758.1624            1.60s\n",
      "        99         758.1132            0.80s\n",
      "       100         758.0375            0.00s\n",
      "Searching best Hyperparameters for q8\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         411.6186            1.53m\n",
      "         2         401.6733            1.45m\n",
      "         3         395.0899            1.43m\n",
      "         4         390.7069            1.42m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         5         387.5727            1.38m\n",
      "         6         385.3920            1.38m\n",
      "         7         383.8750            1.37m\n",
      "         8         382.5357            1.35m\n",
      "         9         381.6574            1.32m\n",
      "        10         380.7628            1.30m\n",
      "        11         380.2105            1.27m\n",
      "        12         379.6348            1.26m\n",
      "        13         379.0619            1.24m\n",
      "        14         378.6618            1.22m\n",
      "        15         378.3516            1.20m\n",
      "        16         378.1170            1.18m\n",
      "        17         377.7497            1.16m\n",
      "        18         377.5388            1.15m\n",
      "        19         377.4034            1.13m\n",
      "        20         377.1901            1.12m\n",
      "        21         377.0383            1.10m\n",
      "        22         376.8665            1.08m\n",
      "        23         376.7898            1.06m\n",
      "        24         376.6759            1.04m\n",
      "        25         376.4847            1.03m\n",
      "        26         376.3660            1.01m\n",
      "        27         376.2866           59.57s\n",
      "        28         376.2092           58.55s\n",
      "        29         376.1342           57.69s\n",
      "        30         376.0018           56.75s\n",
      "        31         375.9394           56.09s\n",
      "        32         375.8036           55.32s\n",
      "        33         375.6947           54.31s\n",
      "        34         375.6154           53.54s\n",
      "        35         375.5320           52.75s\n",
      "        36         375.4933           51.81s\n",
      "        37         375.4459           51.03s\n",
      "        38         375.3912           50.27s\n",
      "        39         375.3271           49.78s\n",
      "        40         375.2643           49.06s\n",
      "        41         375.2217           48.31s\n",
      "        42         375.1804           47.62s\n",
      "        43         375.1430           46.84s\n",
      "        44         375.1222           46.21s\n",
      "        45         375.0774           45.54s\n",
      "        46         375.0332           44.71s\n",
      "        47         374.9866           44.04s\n",
      "        48         374.8996           43.28s\n",
      "        49         374.7951           42.52s\n",
      "        50         374.7506           41.88s\n",
      "        51         374.6963           41.17s\n",
      "        52         374.6656           40.40s\n",
      "        53         374.6337           39.61s\n",
      "        54         374.6029           38.82s\n",
      "        55         374.5711           38.02s\n",
      "        56         374.5549           37.19s\n",
      "        57         374.5099           36.36s\n",
      "        58         374.4858           35.47s\n",
      "        59         374.4384           34.61s\n",
      "        60         374.4088           33.71s\n",
      "        61         374.3912           32.82s\n",
      "        62         374.3346           31.91s\n",
      "        63         374.3169           31.02s\n",
      "        64         374.2909           30.18s\n",
      "        65         374.2706           29.31s\n",
      "        66         374.2265           28.45s\n",
      "        67         374.1641           27.62s\n",
      "        68         374.1354           26.74s\n",
      "        69         374.1101           25.88s\n",
      "        70         374.0958           24.98s\n",
      "        71         374.0469           24.11s\n",
      "        72         374.0236           23.26s\n",
      "        73         373.9967           22.39s\n",
      "        74         373.9636           21.55s\n",
      "        75         373.9553           20.70s\n",
      "        76         373.9493           19.86s\n",
      "        77         373.9448           19.03s\n",
      "        78         373.9368           18.21s\n",
      "        79         373.9270           17.37s\n",
      "        80         373.9238           16.53s\n",
      "        81         373.9018           15.69s\n",
      "        82         373.8550           14.86s\n",
      "        83         373.7993           14.02s\n",
      "        84         373.7672           13.21s\n",
      "        85         373.7224           12.38s\n",
      "        86         373.7120           11.59s\n",
      "        87         373.6977           10.77s\n",
      "        88         373.6866            9.95s\n",
      "        89         373.6794            9.14s\n",
      "        90         373.6728            8.31s\n",
      "        91         373.6625            7.48s\n",
      "        92         373.6550            6.65s\n",
      "        93         373.6481            5.81s\n",
      "        94         373.6388            4.97s\n",
      "        95         373.6324            4.14s\n",
      "        96         373.6021            3.31s\n",
      "        97         373.5604            2.48s\n",
      "        98         373.5140            1.65s\n",
      "        99         373.4953            0.83s\n",
      "       100         373.4802            0.00s\n",
      "Searching best Hyperparameters for q9\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         181.7407            1.81m\n",
      "         2         177.3101            1.69m\n",
      "         3         174.3892            1.61m\n",
      "         4         172.4261            1.62m\n",
      "         5         171.0674            1.59m\n",
      "         6         169.9659            1.54m\n",
      "         7         169.0866            1.50m\n",
      "         8         168.3530            1.47m\n",
      "         9         167.8397            1.45m\n",
      "        10         167.3994            1.43m\n",
      "        11         166.9700            1.41m\n",
      "        12         166.5381            1.38m\n",
      "        13         166.3238            1.38m\n",
      "        14         166.0641            1.37m\n",
      "        15         165.9052           80.86m\n",
      "        16         165.7491           75.08m\n",
      "        17         165.6291           69.92m\n",
      "        18         165.5274           65.33m\n",
      "        19         165.3977           61.20m\n",
      "        20         165.2827           57.49m\n",
      "        21         165.2028           54.14m\n",
      "        22         165.1136           51.08m\n",
      "        23         165.0338           48.30m\n",
      "        24         164.9675           45.74m\n",
      "        25         164.8866           43.38m\n",
      "        26         164.8382           41.20m\n",
      "        27         164.7732           39.20m\n",
      "        28         164.7190           37.32m\n",
      "        29         164.6695           35.58m\n",
      "        30         164.6153           33.95m\n",
      "        31         164.5594           32.42m\n",
      "        32         164.5237           30.99m\n",
      "        33         164.4960           29.64m\n",
      "        34         164.4583           28.36m\n",
      "        35         164.4297           27.16m\n",
      "        36         164.3964           26.02m\n",
      "        37         164.3600           24.94m\n",
      "        38         164.3358           23.92m\n",
      "        39         164.3038           22.95m\n",
      "        40         164.2812           22.03m\n",
      "        41         164.2664           21.15m\n",
      "        42         164.2337           20.32m\n",
      "        43         164.2235           19.52m\n",
      "        44         164.2064           18.75m\n",
      "        45         164.1896           18.02m\n",
      "        46         164.1765           17.33m\n",
      "        47         164.1581           16.66m\n",
      "        48         164.1395           16.02m\n",
      "        49         164.1081           15.40m\n",
      "        50         164.0843           14.81m\n",
      "        51         164.0727           14.24m\n",
      "        52         164.0557           13.69m\n",
      "        53         164.0323           13.17m\n",
      "        54         164.0209           12.66m\n",
      "        55         164.0047           12.17m\n",
      "        56         163.9898           11.70m\n",
      "        57         163.9803           11.24m\n",
      "        58         163.9669           10.80m\n",
      "        59         163.9503           10.37m\n",
      "        60         163.9291            9.96m\n",
      "        61         163.9075            9.56m\n",
      "        62         163.8916            9.18m\n",
      "        63         163.8758            8.80m\n",
      "        64         163.8662            8.44m\n",
      "        65         163.8606            8.09m\n",
      "        66         163.8549            7.75m\n",
      "        67         163.8216            7.42m\n",
      "        68         163.8048            7.10m\n",
      "        69         163.7979            6.79m\n",
      "        70         163.7591            6.48m\n",
      "        71         163.7527            6.18m\n",
      "        72         163.7472            5.89m\n",
      "        73         163.7375            5.61m\n",
      "        74         163.7273            5.33m\n",
      "        75         163.7223            5.07m\n",
      "        76         163.7082            4.80m\n",
      "        77         163.7027            4.55m\n",
      "        78         163.6972            4.30m\n",
      "        79         163.6936            4.06m\n",
      "        80         163.6904            3.82m\n",
      "        81         163.6861            3.59m\n",
      "        82         163.6796            3.36m\n",
      "        83         163.6730            3.14m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        84         163.6612            2.92m\n",
      "        85         163.6573            2.71m\n",
      "        86         163.6450            2.50m\n",
      "        87         163.6419            2.30m\n",
      "        88         163.6350            2.10m\n",
      "        89         163.6319            1.90m\n",
      "        90         163.6128            1.71m\n",
      "        91         163.6018            1.53m\n",
      "        92         163.5968            1.34m\n",
      "        93         163.5716            1.16m\n",
      "        94         163.5536           59.21s\n",
      "        95         163.5319           48.86s\n",
      "        96         163.5271           38.71s\n",
      "        97         163.5022           28.76s\n",
      "        98         163.4823           18.99s\n",
      "        99         163.4780            9.41s\n",
      "       100         163.4644            0.00s\n"
     ]
    }
   ],
   "source": [
    "fit_params = {\"model__sample_weight\":sample_weights_train}\n",
    "for q in search_cv.keys():\n",
    "    print(f\"Searching best Hyperparameters for {q}\")\n",
    "    search_cv[q].fit(X_train,y_train.values.ravel(),**fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9af9841",
   "metadata": {},
   "source": [
    "## Predict on entire training set and calculate mean_pinball_loss_train for all quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "695292c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q1': 20.92979401072278,\n",
       " 'q2': 78.05309032083764,\n",
       " 'q3': 373.3537340294139,\n",
       " 'q4': 508.633947199487,\n",
       " 'q5': 778.4402932578334,\n",
       " 'q6': 824.3514201131849,\n",
       " 'q7': 758.0374754982063,\n",
       " 'q8': 373.48023079754245,\n",
       " 'q9': 163.4644260464789}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpl_train={}\n",
    "for i,q in enumerate(search_cv.keys()):\n",
    "    y_pred=search_cv[q].predict(X_train)\n",
    "    mpl_train[q]=mean_pinball_loss(y_train.values.ravel(),y_pred,sample_weight=sample_weights_train,alpha=quantiles[i])\n",
    "mpl_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc28edf",
   "metadata": {},
   "source": [
    "## Predict on validation set and calculate mean_pinball_loss for all quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2e56d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q1': 20.675099707149602,\n",
       " 'q2': 77.65826534528892,\n",
       " 'q3': 371.4715509523964,\n",
       " 'q4': 505.7313958445188,\n",
       " 'q5': 772.2780109723352,\n",
       " 'q6': 815.5244500458288,\n",
       " 'q7': 749.2662670196337,\n",
       " 'q8': 366.9791288575568,\n",
       " 'q9': 160.49446550129156}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpl_val={}\n",
    "for i,q in enumerate(search_cv.keys()):\n",
    "    y_pred = search_cv[q].predict(X_val)\n",
    "    mpl_val[q]=mean_pinball_loss(y_val.values.ravel(),y_pred,sample_weight=sample_weights_validation,alpha=quantiles[i])\n",
    "mpl_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049769b",
   "metadata": {},
   "source": [
    "## write mean pinball losses to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "353af066",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpls={}\n",
    "mpls['Data'] = ['mpl_train','mpl_val']\n",
    "for q in mpl_train.keys():\n",
    "    mpls[q]=[mpl_train[q],mpl_val[q]]\n",
    "df = pd.DataFrame(mpls)\n",
    "df.to_csv(\"GBRTQuantileLoss_MeanPinballLoss.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda27ab1",
   "metadata": {},
   "source": [
    "## Predict on test set and create output data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33fcf604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pmoha\\AppData\\Local\\Temp\\ipykernel_27160\\1592062439.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[q] = search_cv[q].predict(X_test)\n",
      "C:\\Users\\pmoha\\AppData\\Local\\Temp\\ipykernel_27160\\1592062439.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[q] = search_cv[q].predict(X_test)\n",
      "C:\\Users\\pmoha\\AppData\\Local\\Temp\\ipykernel_27160\\1592062439.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[q] = search_cv[q].predict(X_test)\n",
      "C:\\Users\\pmoha\\AppData\\Local\\Temp\\ipykernel_27160\\1592062439.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[q] = search_cv[q].predict(X_test)\n",
      "C:\\Users\\pmoha\\AppData\\Local\\Temp\\ipykernel_27160\\1592062439.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[q] = search_cv[q].predict(X_test)\n",
      "C:\\Users\\pmoha\\AppData\\Local\\Temp\\ipykernel_27160\\1592062439.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[q] = search_cv[q].predict(X_test)\n",
      "C:\\Users\\pmoha\\AppData\\Local\\Temp\\ipykernel_27160\\1592062439.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[q] = search_cv[q].predict(X_test)\n",
      "C:\\Users\\pmoha\\AppData\\Local\\Temp\\ipykernel_27160\\1592062439.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[q] = search_cv[q].predict(X_test)\n",
      "C:\\Users\\pmoha\\AppData\\Local\\Temp\\ipykernel_27160\\1592062439.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[q] = search_cv[q].predict(X_test)\n"
     ]
    }
   ],
   "source": [
    "test_df = test[id_column]\n",
    "for q in search_cv.keys():\n",
    "    test_df[q] = search_cv[q].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b742c0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "      <th>q4</th>\n",
       "      <th>q5</th>\n",
       "      <th>q6</th>\n",
       "      <th>q7</th>\n",
       "      <th>q8</th>\n",
       "      <th>q9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>741875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>601.238235</td>\n",
       "      <td>805.506886</td>\n",
       "      <td>1374.088858</td>\n",
       "      <td>2221.848777</td>\n",
       "      <td>2631.538765</td>\n",
       "      <td>4896.297014</td>\n",
       "      <td>8795.021614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>741876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1764.285253</td>\n",
       "      <td>2012.918245</td>\n",
       "      <td>3051.838734</td>\n",
       "      <td>4711.923660</td>\n",
       "      <td>5784.636435</td>\n",
       "      <td>12880.121505</td>\n",
       "      <td>28743.523384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>741877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>443.534995</td>\n",
       "      <td>696.864093</td>\n",
       "      <td>1349.048740</td>\n",
       "      <td>2342.687115</td>\n",
       "      <td>3118.363884</td>\n",
       "      <td>7491.719372</td>\n",
       "      <td>17398.386310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>741878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>167.527269</td>\n",
       "      <td>641.192159</td>\n",
       "      <td>790.636246</td>\n",
       "      <td>1175.736542</td>\n",
       "      <td>1986.878909</td>\n",
       "      <td>2501.043503</td>\n",
       "      <td>5892.878668</td>\n",
       "      <td>13605.899737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>741879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1109.958673</td>\n",
       "      <td>1345.992511</td>\n",
       "      <td>2166.427062</td>\n",
       "      <td>3130.679186</td>\n",
       "      <td>3616.274401</td>\n",
       "      <td>7320.619769</td>\n",
       "      <td>15775.075134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185464</th>\n",
       "      <td>927339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1352.809243</td>\n",
       "      <td>1801.080360</td>\n",
       "      <td>2836.879092</td>\n",
       "      <td>4254.380864</td>\n",
       "      <td>4923.898906</td>\n",
       "      <td>11370.236226</td>\n",
       "      <td>26676.480706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185465</th>\n",
       "      <td>927340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139.202440</td>\n",
       "      <td>673.926598</td>\n",
       "      <td>881.471025</td>\n",
       "      <td>1416.199220</td>\n",
       "      <td>2067.371828</td>\n",
       "      <td>2650.497919</td>\n",
       "      <td>5290.428336</td>\n",
       "      <td>10899.373029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185466</th>\n",
       "      <td>927341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1122.487605</td>\n",
       "      <td>1465.068938</td>\n",
       "      <td>2427.541224</td>\n",
       "      <td>3901.848468</td>\n",
       "      <td>4840.810456</td>\n",
       "      <td>11196.066565</td>\n",
       "      <td>23679.097172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185467</th>\n",
       "      <td>927342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1548.057016</td>\n",
       "      <td>1891.135630</td>\n",
       "      <td>2695.133694</td>\n",
       "      <td>3984.849454</td>\n",
       "      <td>4820.355623</td>\n",
       "      <td>10801.855581</td>\n",
       "      <td>21401.855531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185468</th>\n",
       "      <td>927343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1171.699164</td>\n",
       "      <td>1468.977409</td>\n",
       "      <td>2361.758834</td>\n",
       "      <td>3679.241825</td>\n",
       "      <td>4298.725236</td>\n",
       "      <td>10171.334960</td>\n",
       "      <td>23529.804572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185469 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id   q1          q2           q3           q4           q5  \\\n",
       "0       741875  0.0    0.000000   601.238235   805.506886  1374.088858   \n",
       "1       741876  0.0    0.000000  1764.285253  2012.918245  3051.838734   \n",
       "2       741877  0.0    0.000000   443.534995   696.864093  1349.048740   \n",
       "3       741878  0.0  167.527269   641.192159   790.636246  1175.736542   \n",
       "4       741879  0.0    0.000000  1109.958673  1345.992511  2166.427062   \n",
       "...        ...  ...         ...          ...          ...          ...   \n",
       "185464  927339  0.0    0.000000  1352.809243  1801.080360  2836.879092   \n",
       "185465  927340  0.0  139.202440   673.926598   881.471025  1416.199220   \n",
       "185466  927341  0.0    0.000000  1122.487605  1465.068938  2427.541224   \n",
       "185467  927342  0.0    0.000000  1548.057016  1891.135630  2695.133694   \n",
       "185468  927343  0.0    0.000000  1171.699164  1468.977409  2361.758834   \n",
       "\n",
       "                 q6           q7            q8            q9  \n",
       "0       2221.848777  2631.538765   4896.297014   8795.021614  \n",
       "1       4711.923660  5784.636435  12880.121505  28743.523384  \n",
       "2       2342.687115  3118.363884   7491.719372  17398.386310  \n",
       "3       1986.878909  2501.043503   5892.878668  13605.899737  \n",
       "4       3130.679186  3616.274401   7320.619769  15775.075134  \n",
       "...             ...          ...           ...           ...  \n",
       "185464  4254.380864  4923.898906  11370.236226  26676.480706  \n",
       "185465  2067.371828  2650.497919   5290.428336  10899.373029  \n",
       "185466  3901.848468  4840.810456  11196.066565  23679.097172  \n",
       "185467  3984.849454  4820.355623  10801.855581  21401.855531  \n",
       "185468  3679.241825  4298.725236  10171.334960  23529.804572  \n",
       "\n",
       "[185469 rows x 10 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e4a44",
   "metadata": {},
   "source": [
    "## write output to test_quantiles_GBRTQuantileLoss.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f2287ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('test_quantiles_GBRTQuantileLoss.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
